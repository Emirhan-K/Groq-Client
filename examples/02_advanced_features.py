#!/usr/bin/env python3
"""
GeliÅŸmiÅŸ Ã–zellikler Ã–rnekleri
=============================

Bu dosya Groq Client'Ä±n geliÅŸmiÅŸ Ã¶zelliklerini gÃ¶sterir:
- Token counting
- Model registry
- Rate limiting
- Queue management
- Advanced text generation
"""

import os
import sys
import time
import asyncio
from typing import List, Dict, Any

# Proje kÃ¶k dizinini Python path'ine ekle
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from client.groq_client import GroqClient
from core.model_registry import ModelRegistry
from core.token_counter import TokenCounter
from core.rate_limit_handler import RateLimitHandler
from core.queue_manager import QueueManager
from handlers.text_generation import TextGenerationHandler
from handlers.speech_to_text import SpeechToTextHandler

def token_counting_examples():
    """Token counting Ã¶rnekleri"""
    print("=" * 60)
    print("ğŸ”¢ TOKEN COUNTING Ã–RNEKLERÄ°")
    print("=" * 60)
    
    api_key = "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"
    client = GroqClient(api_key)
    
    try:
        # 1. Basit token sayÄ±mÄ±
        print("\n1ï¸âƒ£ Basit Token SayÄ±mÄ±:")
        text = "Merhaba dÃ¼nya! Bu bir test metnidir."
        tokens = client.count_tokens(text, "llama3-8b-8192")
        print(f"Metin: '{text}'")
        print(f"Token sayÄ±sÄ±: {tokens}")
        
        # 2. Uzun metin token sayÄ±mÄ±
        print("\n2ï¸âƒ£ Uzun Metin Token SayÄ±mÄ±:")
        long_text = """
        Python, Guido van Rossum tarafÄ±ndan 1991 yÄ±lÄ±nda geliÅŸtirilen yÃ¼ksek seviyeli, 
        genel amaÃ§lÄ± bir programlama dilidir. Python'un tasarÄ±m felsefesi, kodun 
        okunabilirliÄŸini vurgular ve sÃ¶zdizimi, programcÄ±larÄ±n daha az kod yazarak 
        kavramlarÄ± ifade etmelerine olanak tanÄ±r. Python, nesne yÃ¶nelimli, 
        yorumlanmÄ±ÅŸ ve dinamik olarak yazÄ±lmÄ±ÅŸ bir dildir.
        """
        tokens = client.count_tokens(long_text, "llama3-8b-8192")
        print(f"Uzun metin token sayÄ±sÄ±: {tokens}")
        
        # 3. Mesaj token sayÄ±mÄ±
        print("\n3ï¸âƒ£ Mesaj Token SayÄ±mÄ±:")
        # Mesaj listesi yerine string kullan
        message_text = "Sen yardÄ±mcÄ± bir AI'sÄ±n. Python nedir? Python, yÃ¼ksek seviyeli bir programlama dilidir. AvantajlarÄ± nelerdir?"
        
        message_tokens = client.count_tokens(message_text, "llama3-8b-8192")
        print(f"Mesaj metni token sayÄ±sÄ±: {message_tokens}")
        
        # 4. Token limit kontrolÃ¼
        print("\n4ï¸âƒ£ Token Limit KontrolÃ¼:")
        model_info = client.get_model_info("llama3-8b-8192")
        max_tokens = model_info.get('max_tokens', 8192)
        
        print(f"Model: llama3-8b-8192")
        print(f"Maksimum token: {max_tokens}")
        print(f"KullanÄ±labilir token: {max_tokens - message_tokens}")
        
        # 5. FarklÄ± modeller iÃ§in token sayÄ±mÄ±
        print("\n5ï¸âƒ£ FarklÄ± Modeller iÃ§in Token SayÄ±mÄ±:")
        models = ["llama3-8b-8192"]  # Sadece Ã§alÄ±ÅŸan model
        
        for model in models:
            try:
                tokens = client.count_tokens(text, model)
                print(f"{model}: {tokens} token")
            except Exception as e:
                print(f"{model}: Hata - {e}")
        
    except Exception as e:
        print(f"âŒ Token Counting HatasÄ±: {e}")
    finally:
        client.close()

def model_registry_examples():
    """Model registry Ã¶rnekleri"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ MODEL REGISTRY Ã–RNEKLERÄ°")
    print("=" * 60)
    
    api_key = "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"
    client = GroqClient(api_key)
    
    try:
        # 1. TÃ¼m modelleri listele
        print("\n1ï¸âƒ£ TÃ¼m Modeller:")
        all_models = client.get_available_models()
        print(f"Toplam model sayÄ±sÄ±: {len(all_models)}")
        
        # 2. Chat modelleri
        print("\n2ï¸âƒ£ Chat Modelleri:")
        chat_models = client.get_available_models("chat")
        print(f"Chat model sayÄ±sÄ±: {len(chat_models)}")
        for model in chat_models[:5]:  # Ä°lk 5 modeli gÃ¶ster
            print(f"  - {model}")
        
        # 3. STT modelleri
        print("\n3ï¸âƒ£ STT Modelleri:")
        stt_models = client.get_available_models("stt")
        print(f"STT model sayÄ±sÄ±: {len(stt_models)}")
        for model in stt_models:
            print(f"  - {model}")
        
        # 4. Model bilgileri
        print("\n4ï¸âƒ£ Model Bilgileri:")
        for model_id in chat_models[:3]:  # Ä°lk 3 chat modeli
            try:
                model_info = client.get_model_info(model_id)
                print(f"\n{model_id}:")
                print(f"  - Max Tokens: {model_info.get('max_tokens', 'N/A')}")
                print(f"  - Owner: {model_info.get('owner', 'N/A')}")
                print(f"  - Type: {model_info.get('type', 'N/A')}")
            except Exception as e:
                print(f"{model_id}: Hata - {e}")
        
        # 5. Model kategorileri
        print("\n5ï¸âƒ£ Model Kategorileri:")
        categories = {}
        for model in all_models:
            try:
                info = client.get_model_info(model)
                category = info.get('type', 'unknown')
                if category not in categories:
                    categories[category] = []
                categories[category].append(model)
            except:
                pass
        
        for category, models in categories.items():
            print(f"{category}: {len(models)} model")
        
    except Exception as e:
        print(f"âŒ Model Registry HatasÄ±: {e}")
    finally:
        client.close()

def rate_limiting_examples():
    """Rate limiting Ã¶rnekleri"""
    print("\n" + "=" * 60)
    print("â±ï¸ RATE LIMITING Ã–RNEKLERÄ°")
    print("=" * 60)
    
    api_key = "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"
    client = GroqClient(api_key)
    
    try:
        # 1. Rate limit durumu
        print("\n1ï¸âƒ£ Rate Limit Durumu:")
        status = client.get_rate_limit_status()
        print(f"Request Limit: {status['request_limit']}")
        print(f"Request Remaining: {status['request_remaining']}")
        print(f"Token Limit: {status['token_limit']}")
        print(f"Token Remaining: {status['token_remaining']}")
        print(f"Audio Seconds Limit: {status['audio_seconds_limit']}")
        print(f"Audio Seconds Remaining: {status['audio_seconds_remaining']}")
        
        # 2. Rate limit kontrolÃ¼
        print("\n2ï¸âƒ£ Rate Limit KontrolÃ¼:")
        can_proceed = client.rate_limit_handler.can_proceed(tokens=100, requests=1)
        print(f"100 token, 1 request iÃ§in izin: {can_proceed}")
        
        # 3. Rate limit yenileme
        print("\n3ï¸âƒ£ Rate Limit Yenileme:")
        needs_refresh = client.rate_limit_handler.needs_refresh()
        print(f"Yenileme gerekli: {needs_refresh}")
        
        # 4. Rate limit deÄŸiÅŸiklik tespiti
        print("\n4ï¸âƒ£ Rate Limit DeÄŸiÅŸiklik Tespiti:")
        mock_headers = {
            'x-ratelimit-limit-requests': '200',
            'x-ratelimit-remaining-requests': '180',
            'x-ratelimit-reset-requests': str(int(time.time()) + 3600)
        }
        
        has_changed = client.rate_limit_handler.has_limits_changed(mock_headers)
        print(f"Limit deÄŸiÅŸikliÄŸi tespit edildi: {has_changed}")
        
        # 5. Rate limit Ã¶zeti
        print("\n5ï¸âƒ£ Rate Limit Ã–zeti:")
        summary = client.rate_limit_handler.get_status_summary()
        print(f"Ã–zet: {summary}")
        
    except Exception as e:
        print(f"âŒ Rate Limiting HatasÄ±: {e}")
    finally:
        client.close()

def queue_management_examples():
    """Queue management Ã¶rnekleri"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ QUEUE MANAGEMENT Ã–RNEKLERÄ°")
    print("=" * 60)
    
    api_key = "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"
    
    try:
        # Queue manager'Ä± doÄŸrudan kullan
        rate_handler = RateLimitHandler()
        queue_manager = QueueManager(rate_handler, max_queue_size=10)
        
        # 1. Queue durumu
        print("\n1ï¸âƒ£ Queue Durumu:")
        status = queue_manager.get_queue_status()
        print(f"Toplam sÄ±ralanan: {status.get('total_queued', 0)}")
        print(f"Toplam iÅŸlenen: {status.get('total_processed', 0)}")
        print(f"Toplam baÅŸarÄ±sÄ±z: {status.get('total_failed', 0)}")
        print(f"Toplam yeniden deneme: {status.get('total_retries', 0)}")
        
        # 2. Queue kapasitesi
        print("\n2ï¸âƒ£ Queue Kapasitesi:")
        print(f"Maksimum queue boyutu: {queue_manager.max_queue_size}")
        print(f"Åu anki queue boyutu: {queue_manager.get_queue_size()}")
        print(f"Queue dolu mu: {queue_manager.is_queue_full()}")
        
        # 3. Rate limit handler durumu
        print("\n3ï¸âƒ£ Rate Limit Handler Durumu:")
        print(f"Rate limit handler aktif: {queue_manager.rate_limit_handler is not None}")
        
    except Exception as e:
        print(f"âŒ Queue Management HatasÄ±: {e}")

def advanced_text_generation():
    """GeliÅŸmiÅŸ text generation Ã¶rnekleri"""
    print("\n" + "=" * 60)
    print("ğŸš€ GELÄ°ÅMÄ°Å TEXT GENERATION")
    print("=" * 60)
    
    api_key = "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"
    client = GroqClient(api_key)
    
    try:
        # 1. Function calling
        print("\n1ï¸âƒ£ Function Calling:")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Belirli bir ÅŸehir iÃ§in hava durumu bilgisi alÄ±r",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "Åehir adÄ±"
                            }
                        },
                        "required": ["location"]
                    }
                }
            }
        ]
        
        response = client.text.generate(
            model="llama3-8b-8192",
            messages=[
                {"role": "user", "content": "Ä°stanbul'da hava nasÄ±l?"}
            ],
            tools=tools,
            max_tokens=200
        )
        
        print("Function calling yanÄ±tÄ±:")
        print(response['choices'][0]['message'])
        
        # 2. JSON mode
        print("\n2ï¸âƒ£ JSON Mode:")
        response = client.text.generate(
            model="llama3-8b-8192",
            prompt="TÃ¼rkiye'nin 3 bÃ¼yÃ¼k ÅŸehrini JSON formatÄ±nda listele:",
            response_format={"type": "json_object"},
            max_tokens=200
        )
        
        print("JSON mode yanÄ±tÄ±:")
        print(response['choices'][0]['message']['content'])
        
        # 3. Parallel requests
        print("\n3ï¸âƒ£ Parallel Requests:")
        import concurrent.futures
        
        def make_request(prompt, model):
            try:
                response = client.text.generate(
                    model=model,
                    prompt=prompt,
                    max_tokens=100
                )
                return f"{model}: {response['choices'][0]['message']['content'][:50]}..."
            except Exception as e:
                return f"{model}: Hata - {e}"
        
        prompts = [
            "Python nedir?",
            "JavaScript nedir?",
            "Machine Learning nedir?"
        ]
        
        models = ["llama3-8b-8192"]  # Sadece Ã§alÄ±ÅŸan model
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for prompt in prompts:
                for model in models:
                    future = executor.submit(make_request, prompt, model)
                    futures.append(future)
            
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                print(result)
        
        # 4. Usage tracking
        print("\n4ï¸âƒ£ Usage Tracking:")
        response = client.text.generate(
            model="llama3-8b-8192",
            prompt="KÄ±sa bir hikaye anlat:",
            max_tokens=150
        )
        
        usage = response['usage']
        print(f"Prompt tokens: {usage['prompt_tokens']}")
        print(f"Completion tokens: {usage['completion_tokens']}")
        print(f"Total tokens: {usage['total_tokens']}")
        
        # 5. Error handling with retries
        print("\n5ï¸âƒ£ Error Handling with Retries:")
        
        def retry_request(max_retries=3):
            for attempt in range(max_retries):
                try:
                    response = client.text.generate(
                        model="llama3-8b-8192",
                        prompt="Test mesajÄ±",
                        max_tokens=50
                    )
                    print(f"âœ… BaÅŸarÄ±lÄ± (deneme {attempt + 1})")
                    return response
                except Exception as e:
                    print(f"âŒ Deneme {attempt + 1} baÅŸarÄ±sÄ±z: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(1)  # 1 saniye bekle
                    else:
                        print("Maksimum deneme sayÄ±sÄ±na ulaÅŸÄ±ldÄ±")
                        raise
        
        try:
            retry_request()
        except Exception as e:
            print(f"Retry baÅŸarÄ±sÄ±z: {e}")
        
    except Exception as e:
        print(f"âŒ Advanced Text Generation HatasÄ±: {e}")
    finally:
        client.close()

def main():
    """Ana fonksiyon"""
    print("ğŸš€ GROQ CLIENT - GELÄ°ÅMÄ°Å Ã–ZELLÄ°KLER Ã–RNEKLERÄ°")
    print("=" * 60)
    
    # GeliÅŸmiÅŸ Ã¶rnekler
    token_counting_examples()
    model_registry_examples()
    rate_limiting_examples()
    queue_management_examples()
    advanced_text_generation()
    
    print("\n" + "=" * 60)
    print("âœ… GELÄ°ÅMÄ°Å Ã–ZELLÄ°KLER Ã–RNEKLERÄ° TAMAMLANDI")
    print("=" * 60)

if __name__ == "__main__":
    main() 