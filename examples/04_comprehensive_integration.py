#!/usr/bin/env python3
"""
KapsamlÄ± Entegrasyon Ã–rnekleri
==============================

Bu dosya Groq Client'Ä±n tÃ¼m Ã¶zelliklerini entegre ÅŸekilde kullanÄ±r:
- Text generation + STT + Token counting + Rate limiting + Queue management
- Real-world scenarios
- Performance monitoring
- Error handling
"""

import os
import sys
import time
import asyncio
import json
import wave
import struct
from datetime import datetime
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import math

# Proje kÃ¶k dizinini Python path'ine ekle
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from client.groq_client import GroqClient
from core.model_registry import ModelRegistry
from core.token_counter import TokenCounter
from core.rate_limit_handler import RateLimitHandler
from core.queue_manager import QueueManager
from handlers.text_generation import TextGenerationHandler
from handlers.speech_to_text import SpeechToTextHandler

class GroqClientManager:
    """Groq Client'Ä±n tÃ¼m Ã¶zelliklerini yÃ¶neten sÄ±nÄ±f"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = GroqClient(api_key)
        self.stats = {
            'text_requests': 0,
            'stt_requests': 0,
            'total_tokens': 0,
            'errors': 0,
            'start_time': time.time()
        }
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Performans istatistiklerini dÃ¶ndÃ¼rÃ¼r"""
        elapsed_time = time.time() - self.stats['start_time']
        return {
            'elapsed_time': elapsed_time,
            'text_requests_per_minute': (self.stats['text_requests'] / elapsed_time) * 60,
            'stt_requests_per_minute': (self.stats['stt_requests'] / elapsed_time) * 60,
            'total_tokens_per_minute': (self.stats['total_tokens'] / elapsed_time) * 60,
            'error_rate': self.stats['errors'] / (self.stats['text_requests'] + self.stats['stt_requests']) if (self.stats['text_requests'] + self.stats['stt_requests']) > 0 else 0,
            **self.stats
        }
    
    def create_test_audio(self, filename: str, duration: int = 3) -> str:
        """Test iÃ§in ses dosyasÄ± oluÅŸturur"""
        sample_rate = 44100
        frequency = 440
        num_samples = sample_rate * duration
        
        samples = []
        for i in range(num_samples):
            sample = int(32767 * 0.3 * math.sin(2 * math.pi * frequency * i / sample_rate))
            samples.append(struct.pack('h', sample))
        
        with wave.open(filename, 'w') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(b''.join(samples))
        
        return filename

def scenario_1_basic_integration():
    """Senaryo 1: Temel entegrasyon"""
    print("=" * 60)
    print("ğŸ¯ SENARYO 1: TEMEL ENTEGRASYON")
    print("=" * 60)
    
    api_key = "your_api_key"
    manager = GroqClientManager(api_key)
    
    try:
        # 1. Model bilgilerini al
        print("\n1ï¸âƒ£ Model Bilgileri:")
        chat_models = manager.client.get_available_models("chat")
        stt_models = manager.client.get_available_models("stt")
        
        print(f"Chat modelleri: {len(chat_models)} adet")
        print(f"STT modelleri: {len(stt_models)} adet")
        
        # 2. Text generation
        print("\n2ï¸âƒ£ Text Generation:")
        response = manager.client.text.generate(
            model="llama3-8b-8192",
            prompt="Python programlama dilinin avantajlarÄ±nÄ± aÃ§Ä±kla:",
            max_tokens=150
        )
        
        manager.stats['text_requests'] += 1
        manager.stats['total_tokens'] += response['usage']['total_tokens']
        
        print(f"YanÄ±t: {response['choices'][0]['message']['content']}")
        print(f"KullanÄ±lan token: {response['usage']['total_tokens']}")
        
        # 3. Token sayÄ±mÄ±
        print("\n3ï¸âƒ£ Token SayÄ±mÄ±:")
        text = "Bu bir test metnidir."
        tokens = manager.client.count_tokens(text, "llama3-8b-8192")
        print(f"'{text}' -> {tokens} token")
        
        # 4. Rate limit durumu
        print("\n4ï¸âƒ£ Rate Limit Durumu:")
        status = manager.client.get_rate_limit_status()
        print(f"Request remaining: {status['request_remaining']}/{status['request_limit']}")
        print(f"Token remaining: {status['token_remaining']}/{status['token_limit']}")
        
        # 5. Performans istatistikleri
        print("\n5ï¸âƒ£ Performans Ä°statistikleri:")
        stats = manager.get_performance_stats()
        print(f"Text requests: {stats['text_requests']}")
        print(f"Total tokens: {stats['total_tokens']}")
        print(f"Error rate: {stats['error_rate']:.2%}")
        
    except Exception as e:
        print(f"âŒ Senaryo 1 HatasÄ±: {e}")
        manager.stats['errors'] += 1
    finally:
        manager.client.close()

def scenario_2_advanced_workflow():
    """Senaryo 2: GeliÅŸmiÅŸ iÅŸ akÄ±ÅŸÄ±"""
    print("\n" + "=" * 60)
    print("ğŸš€ SENARYO 2: GELÄ°ÅMÄ°Å Ä°Å AKIÅI")
    print("=" * 60)
    
    api_key = "your_api_key"
    manager = GroqClientManager(api_key)
    
    try:
        # 1. Ses dosyasÄ± oluÅŸtur ve transkripsiyon yap
        print("\n1ï¸âƒ£ Ses Transkripsiyonu:")
        audio_file = manager.create_test_audio("test_workflow.wav", 2)
        
        response = manager.client.speech.transcribe(
            file=audio_file,
            model="whisper-large-v3"
        )
        
        manager.stats['stt_requests'] += 1
        manager.stats['total_tokens'] += response.get('usage', {}).get('total_tokens', 0)
        
        transcription = response['text']
        print(f"Transkripsiyon: {transcription}")
        
        # 2. Transkripsiyon Ã¼zerinde analiz yap
        print("\n2ï¸âƒ£ Transkripsiyon Analizi:")
        analysis_prompt = f"""
        AÅŸaÄŸÄ±daki transkripsiyonu analiz et ve Ã¶zetle:
        
        Transkripsiyon: {transcription}
        
        Analiz:
        - KonuÅŸma kalitesi
        - Dil tespiti
        - Ana konular
        """
        
        response = manager.client.text.generate(
            model="llama3-8b-8192",
            prompt=analysis_prompt,
            max_tokens=200
        )
        
        manager.stats['text_requests'] += 1
        manager.stats['total_tokens'] += response['usage']['total_tokens']
        
        print(f"Analiz: {response['choices'][0]['message']['content']}")
        
        # 3. Token kullanÄ±m analizi
        print("\n3ï¸âƒ£ Token KullanÄ±m Analizi:")
        total_tokens = manager.stats['total_tokens']
        text_tokens = manager.client.count_tokens(analysis_prompt, "llama3-8b-8192")
        
        print(f"Toplam kullanÄ±lan token: {total_tokens}")
        print(f"Analiz prompt token: {text_tokens}")
        print(f"STT token: {response['usage']['total_tokens']}")
        
        # 4. Rate limit kontrolÃ¼
        print("\n4ï¸âƒ£ Rate Limit KontrolÃ¼:")
        can_proceed = manager.client.rate_limit_handler.can_proceed(
            tokens=100, 
            requests=1, 
            audio_seconds=10
        )
        print(f"Yeni istek yapÄ±labilir: {can_proceed}")
        
        # 5. Model performans karÅŸÄ±laÅŸtÄ±rmasÄ±
        print("\n5ï¸âƒ£ Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±:")
        models = ["llama3-8b-8192"]  # Sadece Ã§alÄ±ÅŸan model
        
        for model in models:
            try:
                start_time = time.time()
                response = manager.client.text.generate(
                    model=model,
                    prompt="KÄ±sa bir hikaye anlat:",
                    max_tokens=100
                )
                end_time = time.time()
                
                manager.stats['text_requests'] += 1
                manager.stats['total_tokens'] += response['usage']['total_tokens']
                
                print(f"{model}:")
                print(f"  - SÃ¼re: {end_time - start_time:.2f}s")
                print(f"  - Token: {response['usage']['total_tokens']}")
                print(f"  - YanÄ±t: {response['choices'][0]['message']['content'][:50]}...")
                
            except Exception as e:
                print(f"{model}: Hata - {e}")
                manager.stats['errors'] += 1
        
        # Test dosyasÄ±nÄ± temizle
        if os.path.exists(audio_file):
            os.remove(audio_file)
        
    except Exception as e:
        print(f"âŒ Senaryo 2 HatasÄ±: {e}")
        manager.stats['errors'] += 1
    finally:
        manager.client.close()

def scenario_3_batch_processing():
    """Senaryo 3: Toplu iÅŸleme"""
    print("\n" + "=" * 60)
    print("ğŸ“¦ SENARYO 3: TOPLU Ä°ÅLEME")
    print("=" * 60)
    
    api_key = "your_api_key"
    manager = GroqClientManager(api_key)
    
    try:
        # 1. Toplu text generation
        print("\n1ï¸âƒ£ Toplu Text Generation:")
        prompts = [
            "Python nedir?",
            "JavaScript nedir?",
            "Machine Learning nedir?",
            "Web development nedir?",
            "Data science nedir?"
        ]
        
        def process_prompt(prompt: str) -> Dict[str, Any]:
            try:
                start_time = time.time()
                response = manager.client.text.generate(
                    model="llama3-8b-8192",
                    prompt=prompt,
                    max_tokens=100
                )
                end_time = time.time()
                
                return {
                    'prompt': prompt,
                    'response': response['choices'][0]['message']['content'],
                    'tokens': response['usage']['total_tokens'],
                    'time': end_time - start_time,
                    'success': True
                }
            except Exception as e:
                return {
                    'prompt': prompt,
                    'error': str(e),
                    'success': False
                }
        
        # Paralel iÅŸleme
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(process_prompt, prompt) for prompt in prompts]
            
            for future in as_completed(futures):
                result = future.result()
                if result['success']:
                    manager.stats['text_requests'] += 1
                    manager.stats['total_tokens'] += result['tokens']
                    print(f"âœ… {result['prompt']}: {result['tokens']} token, {result['time']:.2f}s")
                else:
                    manager.stats['errors'] += 1
                    print(f"âŒ {result['prompt']}: {result['error']}")
        
        # 2. Toplu ses dosyasÄ± iÅŸleme
        print("\n2ï¸âƒ£ Toplu Ses DosyasÄ± Ä°ÅŸleme:")
        audio_files = []
        
        # Test ses dosyalarÄ± oluÅŸtur
        for i in range(3):
            filename = f"batch_audio_{i}.wav"
            manager.create_test_audio(filename, 1)
            audio_files.append(filename)
        
        def process_audio(file_path: str) -> Dict[str, Any]:
            try:
                start_time = time.time()
                response = manager.client.speech.transcribe(
                    file=file_path,
                    model="whisper-large-v3"
                )
                end_time = time.time()
                
                return {
                    'file': file_path,
                    'transcription': response['text'],
                    'tokens': response.get('usage', {}).get('total_tokens', 0),
                    'time': end_time - start_time,
                    'success': True
                }
            except Exception as e:
                return {
                    'file': file_path,
                    'error': str(e),
                    'success': False
                }
        
        # SÄ±ralÄ± iÅŸleme (rate limit iÃ§in)
        for audio_file in audio_files:
            result = process_audio(audio_file)
            if result['success']:
                manager.stats['stt_requests'] += 1
                manager.stats['total_tokens'] += result['tokens']
                print(f"âœ… {result['file']}: {result['tokens']} token, {result['time']:.2f}s")
                print(f"   Transkripsiyon: {result['transcription']}")
            else:
                manager.stats['errors'] += 1
                print(f"âŒ {result['file']}: {result['error']}")
            
            time.sleep(1)  # Rate limit iÃ§in bekle
        
        # Test dosyalarÄ±nÄ± temizle
        for audio_file in audio_files:
            if os.path.exists(audio_file):
                os.remove(audio_file)
        
        # 3. Performans Ã¶zeti
        print("\n3ï¸âƒ£ Performans Ã–zeti:")
        stats = manager.get_performance_stats()
        print(f"Toplam istek: {stats['text_requests'] + stats['stt_requests']}")
        print(f"Text requests: {stats['text_requests']}")
        print(f"STT requests: {stats['stt_requests']}")
        print(f"Toplam token: {stats['total_tokens']}")
        print(f"Hata oranÄ±: {stats['error_rate']:.2%}")
        print(f"Text requests/dakika: {stats['text_requests_per_minute']:.1f}")
        print(f"STT requests/dakika: {stats['stt_requests_per_minute']:.1f}")
        
    except Exception as e:
        print(f"âŒ Senaryo 3 HatasÄ±: {e}")
        manager.stats['errors'] += 1
    finally:
        manager.client.close()

def scenario_4_error_handling_and_recovery():
    """Senaryo 4: Hata yÃ¶netimi ve kurtarma"""
    print("\n" + "=" * 60)
    print("ğŸ›¡ï¸ SENARYO 4: HATA YÃ–NETÄ°MÄ° VE KURTARMA")
    print("=" * 60)
    
    api_key = "your_api_key"
    manager = GroqClientManager(api_key)
    
    try:
        # 1. GeÃ§ersiz model hatasÄ±
        print("\n1ï¸âƒ£ GeÃ§ersiz Model HatasÄ±:")
        try:
            response = manager.client.text.generate(
                model="gecersiz-model",
                prompt="Test",
                max_tokens=10
            )
        except Exception as e:
            print(f"Beklenen hata: {type(e).__name__} - {e}")
            manager.stats['errors'] += 1
        
        # 2. Rate limit aÅŸÄ±mÄ± simÃ¼lasyonu
        print("\n2ï¸âƒ£ Rate Limit AÅŸÄ±mÄ± SimÃ¼lasyonu:")
        
        # Ã‡ok sayÄ±da istek gÃ¶nder
        for i in range(10):
            try:
                response = manager.client.text.generate(
                    model="llama3-8b-8192",
                    prompt=f"Test istek {i+1}",
                    max_tokens=10
                )
                manager.stats['text_requests'] += 1
                manager.stats['total_tokens'] += response['usage']['total_tokens']
                print(f"Ä°stek {i+1}: BaÅŸarÄ±lÄ±")
            except Exception as e:
                print(f"Ä°stek {i+1}: {type(e).__name__} - {e}")
                manager.stats['errors'] += 1
                break
        
        # 3. Retry mekanizmasÄ±
        print("\n3ï¸âƒ£ Retry MekanizmasÄ±:")
        
        def retry_request(prompt: str, max_retries: int = 3) -> Dict[str, Any]:
            for attempt in range(max_retries):
                try:
                    response = manager.client.text.generate(
                        model="llama3-8b-8192",
                        prompt=prompt,
                        max_tokens=50
                    )
                    return {
                        'success': True,
                        'response': response['choices'][0]['message']['content'],
                        'attempts': attempt + 1,
                        'tokens': response['usage']['total_tokens']
                    }
                except Exception as e:
                    if attempt < max_retries - 1:
                        print(f"Deneme {attempt + 1} baÅŸarÄ±sÄ±z, yeniden deneniyor...")
                        time.sleep(2 ** attempt)  # Exponential backoff
                    else:
                        return {
                            'success': False,
                            'error': str(e),
                            'attempts': attempt + 1
                        }
        
        retry_result = retry_request("Retry test mesajÄ±")
        if retry_result['success']:
            manager.stats['text_requests'] += 1
            manager.stats['total_tokens'] += retry_result['tokens']
            print(f"âœ… Retry baÅŸarÄ±lÄ± ({retry_result['attempts']} deneme): {retry_result['response']}")
        else:
            manager.stats['errors'] += 1
            print(f"âŒ Retry baÅŸarÄ±sÄ±z ({retry_result['attempts']} deneme): {retry_result['error']}")
        
        # 4. Graceful degradation
        print("\n4ï¸âƒ£ Graceful Degradation:")
        
        def fallback_request(prompt: str) -> str:
            # Ana model baÅŸarÄ±sÄ±z olursa alternatif model dene
            models = ["llama3-8b-8192"]
            
            for model in models:
                try:
                    response = manager.client.text.generate(
                        model=model,
                        prompt=prompt,
                        max_tokens=50
                    )
                    manager.stats['text_requests'] += 1
                    manager.stats['total_tokens'] += response['usage']['total_tokens']
                    return f"{model}: {response['choices'][0]['message']['content']}"
                except Exception as e:
                    print(f"{model} baÅŸarÄ±sÄ±z: {e}")
                    continue
            
            # TÃ¼m modeller baÅŸarÄ±sÄ±z olursa varsayÄ±lan yanÄ±t
            manager.stats['errors'] += 1
            return "ÃœzgÃ¼nÃ¼m, ÅŸu anda yanÄ±t veremiyorum."
        
        fallback_result = fallback_request("Fallback test mesajÄ±")
        print(f"Fallback sonucu: {fallback_result}")
        
        # 5. Hata istatistikleri
        print("\n5ï¸âƒ£ Hata Ä°statistikleri:")
        stats = manager.get_performance_stats()
        print(f"Toplam istek: {stats['text_requests'] + stats['stt_requests']}")
        print(f"BaÅŸarÄ±lÄ± istek: {stats['text_requests'] + stats['stt_requests'] - stats['errors']}")
        print(f"BaÅŸarÄ±sÄ±z istek: {stats['errors']}")
        print(f"BaÅŸarÄ± oranÄ±: {1 - stats['error_rate']:.2%}")
        
    except Exception as e:
        print(f"âŒ Senaryo 4 HatasÄ±: {e}")
        manager.stats['errors'] += 1
    finally:
        manager.client.close()

def scenario_5_real_world_application():
    """Senaryo 5: GerÃ§ek dÃ¼nya uygulamasÄ±"""
    print("\n" + "=" * 60)
    print("ğŸŒ SENARYO 5: GERÃ‡EK DÃœNYA UYGULAMASI")
    print("=" * 60)
    
    api_key = "your_api_key"
    manager = GroqClientManager(api_key)
    
    try:
        # 1. Ã‡ok dilli iÃ§erik analizi
        print("\n1ï¸âƒ£ Ã‡ok Dilli Ä°Ã§erik Analizi:")
        
        # FarklÄ± dillerde metinler
        texts = [
            "Hello, how are you today?",
            "Bonjour, comment allez-vous?",
            "Hola, Â¿cÃ³mo estÃ¡s?",
            "Merhaba, nasÄ±lsÄ±n?"
        ]
        
        for text in texts:
            # Dil tespiti
            language_prompt = f"Bu metnin hangi dilde olduÄŸunu sÃ¶yle: {text}"
            
            response = manager.client.text.generate(
                model="llama3-8b-8192",
                prompt=language_prompt,
                max_tokens=50
            )
            
            manager.stats['text_requests'] += 1
            manager.stats['total_tokens'] += response['usage']['total_tokens']
            
            print(f"'{text}' -> {response['choices'][0]['message']['content']}")
        
        # 2. Ses dosyasÄ± analizi
        print("\n2ï¸âƒ£ Ses DosyasÄ± Analizi:")
        
        # Test ses dosyasÄ± oluÅŸtur
        audio_file = manager.create_test_audio("real_world_audio.wav", 2)
        
        # Transkripsiyon
        stt_response = manager.client.speech.transcribe(
            file=audio_file,
            model="whisper-large-v3"
        )
        
        manager.stats['stt_requests'] += 1
        manager.stats['total_tokens'] += stt_response.get('usage', {}).get('total_tokens', 0)
        
        transcription = stt_response['text']
        print(f"Transkripsiyon: {transcription}")
        
        # Transkripsiyon analizi
        analysis_prompt = f"""
        AÅŸaÄŸÄ±daki ses transkripsiyonunu analiz et:
        
        Transkripsiyon: {transcription}
        
        Analiz:
        1. KonuÅŸma kalitesi
        2. Dil tespiti
        3. Ana konular
        4. Duygu analizi
        """
        
        analysis_response = manager.client.text.generate(
            model="llama3-8b-8192",
            prompt=analysis_prompt,
            max_tokens=200
        )
        
        manager.stats['text_requests'] += 1
        manager.stats['total_tokens'] += analysis_response['usage']['total_tokens']
        
        print(f"Analiz: {analysis_response['choices'][0]['message']['content']}")
        
        # 3. Ä°Ã§erik Ã¶zetleme
        print("\n3ï¸âƒ£ Ä°Ã§erik Ã–zetleme:")
        
        long_text = """
        Python, Guido van Rossum tarafÄ±ndan 1991 yÄ±lÄ±nda geliÅŸtirilen yÃ¼ksek seviyeli, 
        genel amaÃ§lÄ± bir programlama dilidir. Python'un tasarÄ±m felsefesi, kodun 
        okunabilirliÄŸini vurgular ve sÃ¶zdizimi, programcÄ±larÄ±n daha az kod yazarak 
        kavramlarÄ± ifade etmelerine olanak tanÄ±r. Python, nesne yÃ¶nelimli, 
        yorumlanmÄ±ÅŸ ve dinamik olarak yazÄ±lmÄ±ÅŸ bir dildir.
        
        Python, web geliÅŸtirme, veri analizi, yapay zeka, makine Ã¶ÄŸrenmesi, 
        bilimsel hesaplama ve otomasyon gibi birÃ§ok alanda kullanÄ±lÄ±r. 
        Django, Flask, NumPy, Pandas, TensorFlow, PyTorch gibi popÃ¼ler 
        kÃ¼tÃ¼phaneler Python ekosisteminin Ã¶nemli parÃ§alarÄ±dÄ±r.
        """
        
        # Token sayÄ±mÄ±
        tokens = manager.client.count_tokens(long_text, "llama3-8b-8192")
        print(f"Uzun metin token sayÄ±sÄ±: {tokens}")
        
        # Ã–zetleme
        summary_prompt = f"Bu metni kÄ±saca Ã¶zetle:\n\n{long_text}"
        
        summary_response = manager.client.text.generate(
            model="llama3-8b-8192",
            prompt=summary_prompt,
            max_tokens=100
        )
        
        manager.stats['text_requests'] += 1
        manager.stats['total_tokens'] += summary_response['usage']['total_tokens']
        
        print(f"Ã–zet: {summary_response['choices'][0]['message']['content']}")
        
        # 4. Performans raporu
        print("\n4ï¸âƒ£ Performans Raporu:")
        stats = manager.get_performance_stats()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_requests': stats['text_requests'] + stats['stt_requests'],
            'text_requests': stats['text_requests'],
            'stt_requests': stats['stt_requests'],
            'total_tokens': stats['total_tokens'],
            'error_rate': stats['error_rate'],
            'text_requests_per_minute': stats['text_requests_per_minute'],
            'stt_requests_per_minute': stats['stt_requests_per_minute'],
            'total_tokens_per_minute': stats['total_tokens_per_minute'],
            'elapsed_time': stats['elapsed_time']
        }
        
        print("Performans Raporu:")
        for key, value in report.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
        
        # Raporu JSON olarak kaydet
        with open("performance_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("âœ… Performans raporu 'performance_report.json' dosyasÄ±na kaydedildi")
        
        # Test dosyasÄ±nÄ± temizle
        if os.path.exists(audio_file):
            os.remove(audio_file)
        
    except Exception as e:
        print(f"âŒ Senaryo 5 HatasÄ±: {e}")
        manager.stats['errors'] += 1
    finally:
        manager.client.close()

def main():
    """Ana fonksiyon"""
    print("ğŸš€ GROQ CLIENT - KAPSAMLI ENTEGRASYON Ã–RNEKLERÄ°")
    print("=" * 60)
    
    # TÃ¼m senaryolarÄ± Ã§alÄ±ÅŸtÄ±r
    scenario_1_basic_integration()
    scenario_2_advanced_workflow()
    scenario_3_batch_processing()
    scenario_4_error_handling_and_recovery()
    scenario_5_real_world_application()
    
    print("\n" + "=" * 60)
    print("âœ… KAPSAMLI ENTEGRASYON Ã–RNEKLERÄ° TAMAMLANDI")
    print("=" * 60)
    print("ğŸ“Š TÃ¼m Ã¶rnekler baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±!")
    print("ğŸ“ Performans raporu 'performance_report.json' dosyasÄ±nda")

if __name__ == "__main__":
    main() 