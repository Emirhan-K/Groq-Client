# ğŸ¯ GroqClient API Reference

## Genel BakÄ±ÅŸ

`GroqClient`, Groq API ile etkileÅŸim kurmak iÃ§in tasarlanmÄ±ÅŸ ana istemci sÄ±nÄ±fÄ±dÄ±r. TÃ¼m bileÅŸenleri koordine eder ve kullanÄ±cÄ± dostu bir arayÃ¼z saÄŸlar.

## ğŸ“‹ SÄ±nÄ±f TanÄ±mÄ±

```python
class GroqClient:
    """Groq API ile etkileÅŸim iÃ§in ana istemci sÄ±nÄ±fÄ±"""
```

## ğŸ”§ Constructor

### `__init__(api_key: str, base_url: str = "https://api.groq.com")`

Ana istemciyi baÅŸlatÄ±r.

#### Parametreler

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `api_key` | `str` | **Gerekli** | Groq API anahtarÄ± |
| `base_url` | `str` | `"https://api.groq.com"` | API base URL'i |

#### Ã–rnek

```python
from client.groq_client import GroqClient

# Temel kullanÄ±m
client = GroqClient("your-api-key")

# Ã–zel base URL ile
client = GroqClient(
    api_key="your-api-key",
    base_url="https://api.groq.com"
)
```

#### Hatalar

- `ValidationError`: GeÃ§ersiz parametreler
- `ConfigurationError`: YapÄ±landÄ±rma hatasÄ±
- `ClientInitializationError`: BaÅŸlatma hatasÄ±

## ğŸ¯ Properties

### `text` â†’ `TextGenerationHandler`

Text generation handler'Ä±na eriÅŸim saÄŸlar.

```python
# Text generation
response = client.text.generate(
    model="llama3-8b-8192",
    messages=[{"role": "user", "content": "Merhaba!"}],
    max_tokens=100
)
```

### `speech` â†’ `SpeechToTextHandler`

Speech-to-text handler'Ä±na eriÅŸim saÄŸlar.

```python
# Speech-to-text
response = client.speech.transcribe(
    file="audio.mp3",
    model="whisper-large-v3"
)
```

### `rate_limit_handler` â†’ `RateLimitHandler`

Rate limit handler'Ä±na eriÅŸim saÄŸlar.

```python
# Rate limit durumu
status = client.rate_limit_handler.get_status()
print(f"Request remaining: {status['request_remaining']}")
```

## ğŸ”¤ Text Generation Methods

### `text.generate(model: str, prompt: str = None, messages: List[Dict[str, str]] = None, **kwargs) â†’ Dict[str, Any]`

Text tamamlamasÄ± oluÅŸturur.

#### Parametreler

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `model` | `str` | **Gerekli** | KullanÄ±lacak model adÄ± |
| `prompt` | `str` | `None` | Tek satÄ±rlÄ±k prompt |
| `messages` | `List[Dict[str, str]]` | `None` | Mesaj listesi |
| `max_tokens` | `int` | `None` | Maksimum token sayÄ±sÄ± |
| `temperature` | `float` | `None` | YaratÄ±cÄ±lÄ±k seviyesi (0-2) |
| `top_p` | `float` | `None` | Nucleus sampling parametresi |
| `stream` | `bool` | `False` | Streaming yanÄ±t |

#### Ã–rnek

```python
# Prompt ile
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="Python programlama dilini aÃ§Ä±kla",
    max_tokens=200,
    temperature=0.7
)

# Mesaj listesi ile
messages = [
    {"role": "system", "content": "Sen yardÄ±mcÄ± bir asistansÄ±n."},
    {"role": "user", "content": "Merhaba!"},
    {"role": "assistant", "content": "Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?"},
    {"role": "user", "content": "Python nedir?"}
]

response = client.text.generate(
    model="llama3-8b-8192",
    messages=messages,
    max_tokens=150
)

print(response['choices'][0]['message']['content'])
```

#### DÃ¶nen DeÄŸer

```python
{
    'id': 'chatcmpl-...',
    'object': 'chat.completion',
    'created': 1750393319,
    'model': 'llama3-8b-8192',
    'choices': [
        {
            'index': 0,
            'message': {
                'role': 'assistant',
                'content': 'YanÄ±t metni...'
            },
            'finish_reason': 'stop'
        }
    ],
    'usage': {
        'prompt_tokens': 21,
        'completion_tokens': 50,
        'total_tokens': 71
    }
}
```

## ğŸ¤ Speech-to-Text Methods

### `speech.transcribe(file: Union[str, Path], model: str, **kwargs) â†’ Dict[str, Any]`

Ses dosyasÄ±nÄ± yazÄ±ya Ã§evirir.

#### Parametreler

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `file` | `Union[str, Path]` | **Gerekli** | Ses dosyasÄ± yolu |
| `model` | `str` | **Gerekli** | STT model adÄ± |
| `language` | `str` | `None` | Dil kodu (tr, en, es, vb.) |
| `prompt` | `str` | `None` | Transkripsiyon iÃ§in prompt |
| `response_format` | `str` | `"text"` | YanÄ±t formatÄ± |

#### Ã–rnek

```python
# Temel transkripsiyon
response = client.speech.transcribe(
    file="audio.mp3",
    model="whisper-large-v3"
)

# Dil belirtme ile
response = client.speech.transcribe(
    file="audio.wav",
    model="whisper-large-v3",
    language="tr"
)

# Prompt ile
response = client.speech.transcribe(
    file="audio.m4a",
    model="whisper-large-v3",
    prompt="Bu ses dosyasÄ± teknik bir konuÅŸma iÃ§eriyor"
)

print(response['text'])
```

#### DÃ¶nen DeÄŸer

```python
{
    'text': 'Transkripsiyon metni...',
    'x_groq': {
        'id': 'req_...'
    }
}
```

## ğŸ”¢ Token Management Methods

### `count_tokens(text: str, model: str) â†’ int`

Metnin token sayÄ±sÄ±nÄ± hesaplar.

#### Parametreler

| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `text` | `str` | Hesaplanacak metin |
| `model` | `str` | Model adÄ± |

#### Ã–rnek

```python
tokens = client.count_tokens("Bu bir test metnidir.", "llama3-8b-8192")
print(f"Token sayÄ±sÄ±: {tokens}")
```

### `count_message_tokens(messages: List[Dict[str, str]], model: str) â†’ int`

Mesaj listesinin token sayÄ±sÄ±nÄ± hesaplar.

#### Parametreler

| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `messages` | `List[Dict[str, str]]` | Mesaj listesi |
| `model` | `str` | Model adÄ± |

#### Ã–rnek

```python
messages = [
    {"role": "user", "content": "Merhaba"},
    {"role": "assistant", "content": "Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?"}
]
tokens = client.count_message_tokens(messages, "llama3-8b-8192")
print(f"Mesaj token sayÄ±sÄ±: {tokens}")
```

### `get_usage_info(messages: List[Dict[str, str]], model: str, max_tokens: int = 0) â†’ Dict[str, Any]`

Token kullanÄ±m bilgilerini dÃ¶ndÃ¼rÃ¼r.

#### Parametreler

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `messages` | `List[Dict[str, str]]` | **Gerekli** | Mesaj listesi |
| `model` | `str` | **Gerekli** | Model adÄ± |
| `max_tokens` | `int` | `0` | Maksimum token sayÄ±sÄ± |

#### Ã–rnek

```python
usage_info = client.get_usage_info(messages, "llama3-8b-8192", max_tokens=100)
print(f"Toplam token: {usage_info['total_tokens']}")
print(f"Prompt token: {usage_info['prompt_tokens']}")
print(f"Completion token: {usage_info['completion_tokens']}")
```

## ğŸ“Š Rate Limit Methods

### `get_rate_limit_status() â†’ Dict[str, Any]`

Rate limit durumunu dÃ¶ndÃ¼rÃ¼r.

#### Ã–rnek

```python
status = client.get_rate_limit_status()
print(f"Request remaining: {status['request_remaining']}/{status['request_limit']}")
print(f"Token remaining: {status['token_remaining']}/{status['token_limit']}")
print(f"Audio seconds: {status['audio_seconds_remaining']}/{status['audio_seconds_limit']}")
```

#### DÃ¶nen DeÄŸer

```python
{
    'request_limit': 14400,
    'request_remaining': 14399,
    'request_reset_time': 0,
    'token_limit': 6000,
    'token_remaining': 5987,
    'token_reset_time': 0,
    'audio_seconds_limit': 7200,
    'audio_seconds_remaining': 7200,
    'audio_seconds_reset_time': 0,
    'last_update': 1750393319.341,
    'current_time': 1750393319.343,
    'has_rate_limit_info': True,
    'time_since_update': 0.002
}
```

## ğŸ“‹ Queue Management Methods

### `enqueue_request(request_func, *args, priority: str = "normal", tokens_required: int = 0, max_retries: int = 3, **kwargs) â†’ str`

Ä°steÄŸi sÄ±raya alÄ±r.

#### Parametreler

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `request_func` | `Callable` | **Gerekli** | Ã‡alÄ±ÅŸtÄ±rÄ±lacak fonksiyon |
| `*args` | `tuple` | - | Fonksiyon argÃ¼manlarÄ± |
| `priority` | `str` | `"normal"` | Ã–ncelik seviyesi |
| `tokens_required` | `int` | `0` | Gereken token sayÄ±sÄ± |
| `max_retries` | `int` | `3` | Maksimum yeniden deneme |
| `**kwargs` | `dict` | - | Fonksiyon keyword argÃ¼manlarÄ± |

#### Ã–rnek

```python
request_id = client.enqueue_request(
    client.text.generate,
    model="llama3-8b-8192",
    prompt="Test mesajÄ±",
    priority="high",
    tokens_required=50
)
print(f"Ä°stek ID: {request_id}")
```

### `process_queue() â†’ None`

Ä°stek sÄ±rasÄ±nÄ± iÅŸler.

#### Ã–rnek

```python
client.process_queue()
```

### `get_queue_status() â†’ Dict[str, Any]`

SÄ±ra durumunu dÃ¶ndÃ¼rÃ¼r.

#### Ã–rnek

```python
queue_status = client.get_queue_status()
print(f"Total queued: {queue_status['total_queued']}")
print(f"Processing: {queue_status['processing']}")
```

#### DÃ¶nen DeÄŸer

```python
{
    'queue_sizes': {
        'low': 0,
        'normal': 0,
        'high': 0,
        'urgent': 0
    },
    'total_queued': 0,
    'stats': {
        'total_queued': 0,
        'total_processed': 0,
        'total_failed': 0,
        'total_retries': 0
    },
    'processing': False,
    'max_queue_size': 1000
}
```

### `clear_queue(priority: Optional[str] = None) â†’ None`

SÄ±rayÄ± temizler.

#### Parametreler

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `priority` | `Optional[str]` | `None` | Temizlenecek Ã¶ncelik |

#### Ã–rnek

```python
# TÃ¼m sÄ±rayÄ± temizle
client.clear_queue()

# Sadece dÃ¼ÅŸÃ¼k Ã¶ncelikli istekleri temizle
client.clear_queue(priority="low")
```

### `stop_queue_processing() â†’ None`

SÄ±ra iÅŸlemeyi durdurur.

#### Ã–rnek

```python
client.stop_queue_processing()
```

## ğŸ” Model Information Methods

### `get_available_models(model_type: Optional[str] = None) â†’ List[str]`

KullanÄ±labilir modelleri listeler.

#### Parametreler

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `model_type` | `Optional[str]` | `None` | Model tipi filtresi |

#### Ã–rnek

```python
# TÃ¼m modeller
all_models = client.get_available_models()

# Sadece chat modelleri
chat_models = client.get_available_models("chat")

# Sadece STT modelleri
stt_models = client.get_available_models("stt")

print(f"Chat models: {len(chat_models)}")
print(f"STT models: {len(stt_models)}")
```

### `get_model_info(model: str) â†’ Dict[str, Any]`

Model bilgilerini dÃ¶ndÃ¼rÃ¼r.

#### Parametreler

| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `model` | `str` | Model adÄ± |

#### Ã–rnek

```python
model_info = client.get_model_info("llama3-8b-8192")
print(f"Max tokens: {model_info['max_tokens']}")
print(f"Model type: {model_info['type']}")
```

#### DÃ¶nen DeÄŸer

```python
{
    'id': 'llama3-8b-8192',
    'object': 'model',
    'created': 1703120000,
    'owned_by': 'groq',
    'max_tokens': 8192,
    'type': 'chat',
    'context_length': 8192
}
```

### `is_model_supported(model: str) â†’ bool`

Model'in desteklenip desteklenmediÄŸini kontrol eder.

#### Parametreler

| Parametre | Tip | AÃ§Ä±klama |
|-----------|-----|----------|
| `model` | `str` | Model adÄ± |

#### Ã–rnek

```python
is_supported = client.is_model_supported("llama3-8b-8192")
if is_supported:
    print("Model destekleniyor")
else:
    print("Model desteklenmiyor")
```

## ğŸ”„ Context Manager Methods

### `__enter__() â†’ GroqClient`

Context manager giriÅŸi.

### `__exit__(exc_type, exc_val, exc_tb) â†’ None`

Context manager Ã§Ä±kÄ±ÅŸÄ±.

#### Ã–rnek

```python
with GroqClient("your-api-key") as client:
    response = client.text.generate(
        model="llama3-8b-8192",
        prompt="Merhaba!",
        max_tokens=50
    )
    print(response['choices'][0]['message']['content'])
```

## ğŸ§¹ Cleanup Methods

### `close() â†’ None`

Ä°stemciyi kapatÄ±r ve kaynaklarÄ± temizler.

#### Ã–rnek

```python
client = GroqClient("your-api-key")
try:
    # Ä°stemci kullanÄ±mÄ±
    response = client.text.generate(...)
finally:
    client.close()
```

## ğŸš¨ Error Handling

### YaygÄ±n Hatalar

#### `ValidationError`
GeÃ§ersiz parametreler iÃ§in.

```python
try:
    client = GroqClient("")  # BoÅŸ API key
except ValidationError as e:
    print(f"Validation error: {e}")
```

#### `AuthenticationError`
Kimlik doÄŸrulama hatasÄ±.

```python
try:
    response = client.text.generate(...)
except AuthenticationError as e:
    print(f"Authentication error: {e}")
```

#### `RateLimitExceeded`
Rate limit aÅŸÄ±mÄ±.

```python
try:
    response = client.text.generate(...)
except RateLimitExceeded as e:
    print(f"Rate limit exceeded: {e}")
    # Otomatik bekleme yapÄ±lÄ±r
```

#### `InvalidModel`
GeÃ§ersiz model.

```python
try:
    response = client.text.generate(
        model="gecersiz-model",
        prompt="Test"
    )
except InvalidModel as e:
    print(f"Invalid model: {e}")
```

## ğŸ“Š Performance Tips

### 1. **Connection Reuse**
```python
# Ä°stemciyi yeniden kullan
client = GroqClient("your-api-key")

for i in range(10):
    response = client.text.generate(...)
    # Ä°stemci otomatik olarak connection'Ä± yeniden kullanÄ±r
```

### 2. **Batch Processing**
```python
# Toplu iÅŸleme iÃ§in queue kullan
for prompt in prompts:
    client.enqueue_request(
        client.text.generate,
        model="llama3-8b-8192",
        prompt=prompt,
        priority="normal"
    )

client.process_queue()
```

### 3. **Rate Limit Monitoring**
```python
# Rate limit durumunu sÃ¼rekli izle
status = client.get_rate_limit_status()
if status['request_remaining'] < 10:
    print("Rate limit yaklaÅŸÄ±yor!")
```

## ğŸ”§ Advanced Usage

### Custom Configuration
```python
# Ã–zel yapÄ±landÄ±rma ile
client = GroqClient(
    api_key="your-api-key",
    base_url="https://api.groq.com"
)

# Rate limit handler'a eriÅŸim
rate_handler = client.rate_limit_handler
rate_handler.force_refresh(api_callback)
```

### Error Recovery
```python
def safe_generate(client, prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.text.generate(
                model="llama3-8b-8192",
                prompt=prompt,
                max_tokens=100
            )
        except RateLimitExceeded:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            raise
        except Exception as e:
            print(f"Error on attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                raise
```

---

Bu API, **gÃ¼Ã§lÃ¼**, **esnek** ve **kullanÄ±cÄ± dostu** bir arayÃ¼z saÄŸlar. TÃ¼m temel iÅŸlemler iÃ§in optimize edilmiÅŸ metodlar iÃ§erir ve enterprise seviyesinde hata yÃ¶netimi sunar. 