# ğŸ“š Models Reference

## Genel BakÄ±ÅŸ

Bu dokÃ¼mantasyon, Groq API'de kullanÄ±labilir tÃ¼m modellerin detaylÄ± referansÄ±nÄ± iÃ§erir. Her model iÃ§in Ã¶zellikler, kullanÄ±m alanlarÄ± ve performans bilgileri saÄŸlanÄ±r.

## ğŸ¤– Text Generation Models

### Chat Models

#### `llama3-8b-8192`
**En popÃ¼ler ve dengeli model**

- **Tip**: Chat/Completion
- **Parametre SayÄ±sÄ±**: 8B
- **Context Length**: 8,192 tokens
- **Max Output**: 8,192 tokens
- **HÄ±z**: âš¡âš¡âš¡âš¡âš¡ (Ã‡ok hÄ±zlÄ±)
- **Kalite**: â­â­â­â­ (Ä°yi)

**Ã–zellikler:**
- Ã‡ok hÄ±zlÄ± yanÄ±t sÃ¼resi
- Genel amaÃ§lÄ± kullanÄ±m
- TÃ¼rkÃ§e desteÄŸi
- Uygun fiyat

**KullanÄ±m AlanlarÄ±:**
- Genel sohbet
- Kod yazma
- Metin analizi
- HÄ±zlÄ± prototipleme

**Ã–rnek KullanÄ±m:**
```python
response = client.text.generate(
    model="llama3-8b-8192",
    messages=[
        {"role": "user", "content": "Python'da bir web scraper nasÄ±l yazÄ±lÄ±r?"}
    ],
    max_tokens=500,
    temperature=0.7
)
```

#### `llama3-70b-8192`
**YÃ¼ksek kaliteli, bÃ¼yÃ¼k model**

- **Tip**: Chat/Completion
- **Parametre SayÄ±sÄ±**: 70B
- **Context Length**: 8,192 tokens
- **Max Output**: 8,192 tokens
- **HÄ±z**: âš¡âš¡âš¡ (Orta)
- **Kalite**: â­â­â­â­â­ (MÃ¼kemmel)

**Ã–zellikler:**
- YÃ¼ksek kaliteli yanÄ±tlar
- KarmaÅŸÄ±k gÃ¶revler iÃ§in uygun
- DetaylÄ± analiz
- YaratÄ±cÄ± iÃ§erik

**KullanÄ±m AlanlarÄ±:**
- KarmaÅŸÄ±k analizler
- YaratÄ±cÄ± yazÄ±m
- Teknik dokÃ¼mantasyon
- EÄŸitim materyali

**Ã–rnek KullanÄ±m:**
```python
response = client.text.generate(
    model="llama3-70b-8192",
    messages=[
        {"role": "system", "content": "Sen deneyimli bir yazÄ±lÄ±m mimarÄ±sÄ±n."},
        {"role": "user", "content": "Mikroservis mimarisi hakkÄ±nda detaylÄ± bir analiz yap."}
    ],
    max_tokens=1000,
    temperature=0.3
)
```

#### `mixtral-8x7b-32768`
**Uzun context desteÄŸi**

- **Tip**: Chat/Completion
- **Parametre SayÄ±sÄ±**: 47B (8x7B mixture)
- **Context Length**: 32,768 tokens
- **Max Output**: 32,768 tokens
- **HÄ±z**: âš¡âš¡âš¡âš¡ (HÄ±zlÄ±)
- **Kalite**: â­â­â­â­â­ (MÃ¼kemmel)

**Ã–zellikler:**
- Ã‡ok uzun context desteÄŸi
- YÃ¼ksek kalite
- HÄ±zlÄ± yanÄ±t
- Ã‡ok dilli destek

**KullanÄ±m AlanlarÄ±:**
- Uzun dokÃ¼man analizi
- Ã‡ok adÄ±mlÄ± gÃ¶revler
- Ã‡eviri
- Kod analizi

**Ã–rnek KullanÄ±m:**
```python
response = client.text.generate(
    model="mixtral-8x7b-32768",
    messages=[
        {"role": "user", "content": "Bu uzun dokÃ¼manÄ± analiz et ve Ã¶zetle..."}
    ],
    max_tokens=2000,
    temperature=0.5
)
```

#### `gemma2-9b-it`
**Google'Ä±n Gemma modeli**

- **Tip**: Chat/Completion
- **Parametre SayÄ±sÄ±**: 9B
- **Context Length**: 8,192 tokens
- **Max Output**: 8,192 tokens
- **HÄ±z**: âš¡âš¡âš¡âš¡ (HÄ±zlÄ±)
- **Kalite**: â­â­â­â­ (Ä°yi)

**Ã–zellikler:**
- Google'Ä±n aÃ§Ä±k kaynak modeli
- Ä°yi performans
- GÃ¼venli yanÄ±tlar
- Ã‡ok dilli destek

**KullanÄ±m AlanlarÄ±:**
- Genel sohbet
- EÄŸitim
- Ä°Ã§erik oluÅŸturma
- GÃ¼venli uygulamalar

**Ã–rnek KullanÄ±m:**
```python
response = client.text.generate(
    model="gemma2-9b-it",
    messages=[
        {"role": "user", "content": "GÃ¼venli ÅŸifre oluÅŸturma hakkÄ±nda bilgi ver."}
    ],
    max_tokens=300,
    temperature=0.2
)
```

### Completion Models

#### `llama3-8b-8192`
**Completion iÃ§in de kullanÄ±labilir**

```python
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="Python'da bir fonksiyon yaz:",
    max_tokens=200,
    temperature=0.1
)
```

## ğŸ¤ Speech-to-Text Models

### Whisper Models

#### `whisper-large-v3`
**En yÃ¼ksek kaliteli STT modeli**

- **Tip**: Speech-to-Text
- **Dil DesteÄŸi**: 99+ dil
- **Maksimum Dosya Boyutu**: 25MB
- **Desteklenen Formatlar**: mp3, mp4, mpeg, mpga, m4a, wav, webm
- **Kalite**: â­â­â­â­â­ (MÃ¼kemmel)
- **HÄ±z**: âš¡âš¡âš¡ (Orta)

**Ã–zellikler:**
- YÃ¼ksek doÄŸruluk
- Ã‡ok dilli destek
- GÃ¼rÃ¼ltÃ¼ toleransÄ±
- Punctuation desteÄŸi

**KullanÄ±m AlanlarÄ±:**
- Profesyonel transkripsiyon
- Ã‡ok dilli iÃ§erik
- GÃ¼rÃ¼ltÃ¼lÃ¼ ortamlar
- YÃ¼ksek doÄŸruluk gerektiren uygulamalar

**Ã–rnek KullanÄ±m:**
```python
response = client.speech.transcribe(
    file="audio.mp3",
    model="whisper-large-v3",
    language="tr",  # TÃ¼rkÃ§e
    prompt="Bu ses dosyasÄ± teknik bir konuÅŸma iÃ§eriyor"
)
```

#### `whisper-large-v3-turbo`
**HÄ±zlÄ± STT modeli**

- **Tip**: Speech-to-Text
- **Dil DesteÄŸi**: 99+ dil
- **Maksimum Dosya Boyutu**: 25MB
- **Desteklenen Formatlar**: mp3, mp4, mpeg, mpga, m4a, wav, webm
- **Kalite**: â­â­â­â­ (Ä°yi)
- **HÄ±z**: âš¡âš¡âš¡âš¡âš¡ (Ã‡ok hÄ±zlÄ±)

**Ã–zellikler:**
- Ã‡ok hÄ±zlÄ± iÅŸleme
- Ä°yi doÄŸruluk
- GerÃ§ek zamanlÄ± uygulamalar
- DÃ¼ÅŸÃ¼k maliyet

**KullanÄ±m AlanlarÄ±:**
- GerÃ§ek zamanlÄ± transkripsiyon
- CanlÄ± yayÄ±n
- HÄ±zlÄ± not alma
- Mobil uygulamalar

**Ã–rnek KullanÄ±m:**
```python
response = client.speech.transcribe(
    file="audio.wav",
    model="whisper-large-v3-turbo",
    language="en",  # Ä°ngilizce
    response_format="text"
)
```

## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

### Text Generation Models

| Model | Parametre | Context | HÄ±z | Kalite | Fiyat | KullanÄ±m |
|-------|-----------|---------|-----|--------|-------|----------|
| `llama3-8b-8192` | 8B | 8K | âš¡âš¡âš¡âš¡âš¡ | â­â­â­â­ | $ | Genel |
| `llama3-70b-8192` | 70B | 8K | âš¡âš¡âš¡ | â­â­â­â­â­ | $$$ | KarmaÅŸÄ±k |
| `mixtral-8x7b-32768` | 47B | 32K | âš¡âš¡âš¡âš¡ | â­â­â­â­â­ | $$ | Uzun |
| `gemma2-9b-it` | 9B | 8K | âš¡âš¡âš¡âš¡ | â­â­â­â­ | $ | GÃ¼venli |

### Speech-to-Text Models

| Model | Kalite | HÄ±z | Dil | KullanÄ±m |
|-------|--------|-----|-----|----------|
| `whisper-large-v3` | â­â­â­â­â­ | âš¡âš¡âš¡ | 99+ | Profesyonel |
| `whisper-large-v3-turbo` | â­â­â­â­ | âš¡âš¡âš¡âš¡âš¡ | 99+ | HÄ±zlÄ± |

## ğŸ¯ Model SeÃ§im Rehberi

### Text Generation iÃ§in

#### HÄ±zlÄ± ve Ekonomik
```python
# GÃ¼nlÃ¼k kullanÄ±m, hÄ±zlÄ± yanÄ±t
model = "llama3-8b-8192"
```

#### YÃ¼ksek Kalite
```python
# KarmaÅŸÄ±k gÃ¶revler, detaylÄ± analiz
model = "llama3-70b-8192"
```

#### Uzun Context
```python
# Uzun dokÃ¼manlar, Ã§ok adÄ±mlÄ± gÃ¶revler
model = "mixtral-8x7b-32768"
```

#### GÃ¼venli Uygulamalar
```python
# EÄŸitim, gÃ¼venlik odaklÄ±
model = "gemma2-9b-it"
```

### Speech-to-Text iÃ§in

#### Profesyonel KullanÄ±m
```python
# YÃ¼ksek doÄŸruluk gerektiren
model = "whisper-large-v3"
```

#### HÄ±zlÄ± Ä°ÅŸleme
```python
# GerÃ§ek zamanlÄ±, hÄ±zlÄ±
model = "whisper-large-v3-turbo"
```

## ğŸ”§ Model KonfigÃ¼rasyonu

### Temperature AyarlarÄ±

```python
# YaratÄ±cÄ± iÃ§erik (0.7-1.0)
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="YaratÄ±cÄ± bir hikaye yaz",
    temperature=0.8
)

# Dengeli yanÄ±tlar (0.3-0.7)
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="Bir konuyu aÃ§Ä±kla",
    temperature=0.5
)

# Kesin yanÄ±tlar (0.0-0.3)
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="FaktlarÄ± listele",
    temperature=0.1
)
```

### Max Tokens AyarlarÄ±

```python
# KÄ±sa yanÄ±tlar (50-200 tokens)
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="KÄ±sa bir Ã¶zet yaz",
    max_tokens=100
)

# Orta yanÄ±tlar (200-500 tokens)
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="DetaylÄ± aÃ§Ä±klama yap",
    max_tokens=300
)

# Uzun yanÄ±tlar (500+ tokens)
response = client.text.generate(
    model="llama3-8b-8192",
    prompt="KapsamlÄ± analiz yap",
    max_tokens=1000
)
```

## ğŸŒ Dil DesteÄŸi

### Desteklenen Diller

#### Text Generation
- **TÃ¼rkÃ§e**: âœ… MÃ¼kemmel
- **Ä°ngilizce**: âœ… MÃ¼kemmel
- **Almanca**: âœ… Ä°yi
- **FransÄ±zca**: âœ… Ä°yi
- **Ä°spanyolca**: âœ… Ä°yi
- **Ä°talyanca**: âœ… Ä°yi
- **Portekizce**: âœ… Ä°yi
- **RusÃ§a**: âœ… Ä°yi
- **Ã‡ince**: âœ… Ä°yi
- **Japonca**: âœ… Ä°yi
- **Korece**: âœ… Ä°yi
- **ArapÃ§a**: âœ… Ä°yi

#### Speech-to-Text
- **TÃ¼rkÃ§e**: âœ… MÃ¼kemmel
- **Ä°ngilizce**: âœ… MÃ¼kemmel
- **99+ dil**: âœ… Destekleniyor

### Dil Belirtme

```python
# Text generation iÃ§in
messages = [
    {"role": "system", "content": "Sen TÃ¼rkÃ§e konuÅŸan bir asistansÄ±n."},
    {"role": "user", "content": "Merhaba!"}
]

# STT iÃ§in
response = client.speech.transcribe(
    file="audio.mp3",
    model="whisper-large-v3",
    language="tr"  # TÃ¼rkÃ§e
)
```

## ğŸ“ˆ Performans Optimizasyonu

### Model SeÃ§imi

```python
def choose_model(task_type, requirements):
    """GÃ¶rev tipine gÃ¶re model seÃ§imi"""
    
    if task_type == "fast_response":
        return "llama3-8b-8192"
    elif task_type == "high_quality":
        return "llama3-70b-8192"
    elif task_type == "long_context":
        return "mixtral-8x7b-32768"
    elif task_type == "safe":
        return "gemma2-9b-it"
    else:
        return "llama3-8b-8192"  # VarsayÄ±lan
```

### Batch Processing

```python
def batch_process_with_optimal_model(texts):
    """Toplu iÅŸleme iÃ§in optimal model seÃ§imi"""
    
    # KÄ±sa metinler iÃ§in hÄ±zlÄ± model
    short_texts = [t for t in texts if len(t) < 100]
    if short_texts:
        for text in short_texts:
            client.enqueue_request(
                client.text.generate,
                model="llama3-8b-8192",
                prompt=text,
                priority="high"
            )
    
    # Uzun metinler iÃ§in kaliteli model
    long_texts = [t for t in texts if len(t) >= 100]
    if long_texts:
        for text in long_texts:
            client.enqueue_request(
                client.text.generate,
                model="llama3-70b-8192",
                prompt=text,
                priority="normal"
            )
    
    client.process_queue()
```

## ğŸ” Model Bilgilerini Alma

### KullanÄ±labilir Modeller

```python
# TÃ¼m modeller
all_models = client.get_available_models()
print(f"Toplam model sayÄ±sÄ±: {len(all_models)}")

# Chat modelleri
chat_models = client.get_available_models("chat")
print(f"Chat modelleri: {chat_models}")

# STT modelleri
stt_models = client.get_available_models("stt")
print(f"STT modelleri: {stt_models}")
```

### Model DetaylarÄ±

```python
# Model bilgileri
model_info = client.get_model_info("llama3-8b-8192")
print(f"Model ID: {model_info['id']}")
print(f"Max tokens: {model_info['max_tokens']}")
print(f"Model type: {model_info['type']}")
print(f"Context length: {model_info['context_length']}")
```

### Model DesteÄŸi KontrolÃ¼

```python
# Model desteÄŸi
def check_model_support(model_name, task_type):
    """Model desteÄŸini kontrol et"""
    
    if not client.is_model_supported(model_name):
        return False, "Model bulunamadÄ±"
    
    model_info = client.get_model_info(model_name)
    
    if task_type == "chat" and model_info['type'] != "chat":
        return False, "Model chat iÃ§in uygun deÄŸil"
    
    if task_type == "stt" and model_info['type'] != "stt":
        return False, "Model STT iÃ§in uygun deÄŸil"
    
    return True, "Model destekleniyor"

# KullanÄ±m
is_supported, message = check_model_support("llama3-8b-8192", "chat")
print(f"Destek: {is_supported}, Mesaj: {message}")
```

## ğŸš€ Gelecek Modeller

### Planlanan Ã–zellikler

- **Daha bÃ¼yÃ¼k context**: 100K+ token desteÄŸi
- **Ã‡ok modal modeller**: Text + Image + Audio
- **Ã–zel alan modelleri**: TÄ±p, hukuk, finans
- **Daha hÄ±zlÄ± modeller**: Real-time uygulamalar
- **Daha kÃ¼Ã§Ã¼k modeller**: Edge cihazlar iÃ§in

### Model GÃ¼ncellemeleri

```python
# Model gÃ¼ncellemelerini kontrol et
def check_model_updates():
    """Model gÃ¼ncellemelerini kontrol et"""
    
    current_models = set(client.get_available_models())
    cached_models = set(get_cached_model_list())
    
    new_models = current_models - cached_models
    removed_models = cached_models - current_models
    
    if new_models:
        print(f"Yeni modeller: {new_models}")
    
    if removed_models:
        print(f"KaldÄ±rÄ±lan modeller: {removed_models}")
    
    return new_models, removed_models
```

---

Bu referans, tÃ¼m Groq modellerinin detaylÄ± bilgilerini iÃ§erir. Model seÃ§imi yaparken gÃ¶rev gereksinimlerinizi, performans ihtiyaÃ§larÄ±nÄ±zÄ± ve maliyet kÄ±sÄ±tlarÄ±nÄ±zÄ± gÃ¶z Ã¶nÃ¼nde bulundurun. 